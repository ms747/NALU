{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAccumlatorCell(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.W_hat = Parameter(torch.Tensor(in_dim,out_dim))\n",
    "        self.M_hat = Parameter(torch.Tensor(in_dim,out_dim))\n",
    "        self.W  = Parameter(torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat))\n",
    "        self.register_parameter('bias',None)\n",
    "        init.kaiming_uniform_(self.W_hat,a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.M_hat,a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return F.linear(input,self.W,self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nac = NeuralAccumlatorCell(1,1)\n",
    "nac.forward(torch.Tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_hat torch.Size([1, 1])\n",
      "M_hat torch.Size([1, 1])\n",
      "W torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in nac.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAC(nn.Module):\n",
    "    def __init__(self,num_layers,inp_nodes,hide_node,out_nodes):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.inp_nodes = inp_nodes\n",
    "        self.hide_node = hide_node\n",
    "        self.out_nodes = out_nodes\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            layers.append(NeuralAccumlatorCell(hide_node if i > 0 else inp_nodes,hide_node if i < num_layers - 1 else out_nodes))\n",
    "            \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = NAC(3,3,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.W_hat\n",
      "model.0.M_hat\n",
      "model.0.W\n",
      "model.1.W_hat\n",
      "model.1.M_hat\n",
      "model.1.W\n",
      "model.2.W_hat\n",
      "model.2.M_hat\n",
      "model.2.W\n"
     ]
    }
   ],
   "source": [
    "for name,param in z.named_parameters():\n",
    "    if(param.requires_grad):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = to"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
